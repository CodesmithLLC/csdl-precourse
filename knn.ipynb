{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#K-Nearest-Neighbors\" data-toc-modified-id=\"K-Nearest-Neighbors-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>K-Nearest Neighbors</a></span></li><li><span><a href=\"#Preparing-Data\" data-toc-modified-id=\"Preparing-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Preparing Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importing-and-Analyzing-Data\" data-toc-modified-id=\"Importing-and-Analyzing-Data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Importing and Analyzing Data</a></span></li><li><span><a href=\"#Training-and-Testing\" data-toc-modified-id=\"Training-and-Testing-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Training and Testing</a></span></li><li><span><a href=\"#Features-and-Labels\" data-toc-modified-id=\"Features-and-Labels-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Features and Labels</a></span></li><li><span><a href=\"#Coding-Challenge:-train_test_split\" data-toc-modified-id=\"Coding-Challenge:-train_test_split-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Coding Challenge: <code>train_test_split</code></a></span></li></ul></li><li><span><a href=\"#Creating-a-Custom-KNN-Model\" data-toc-modified-id=\"Creating-a-Custom-KNN-Model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Creating a Custom KNN Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Three-Nearest-Neighbors-for-Iris-Classification\" data-toc-modified-id=\"Three-Nearest-Neighbors-for-Iris-Classification-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Three Nearest Neighbors for Iris Classification</a></span></li><li><span><a href=\"#Coding-Challenge:-get_distances\" data-toc-modified-id=\"Coding-Challenge:-get_distances-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Coding Challenge: <code>get_distances</code></a></span></li><li><span><a href=\"#Coding-Challenge:-get_nearest_neighbors\" data-toc-modified-id=\"Coding-Challenge:-get_nearest_neighbors-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Coding Challenge: <code>get_nearest_neighbors</code></a></span></li><li><span><a href=\"#Coding-Challenge:-determine_class\" data-toc-modified-id=\"Coding-Challenge:-determine_class-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Coding Challenge: <code>determine_class</code></a></span></li><li><span><a href=\"#Coding-Challenge:-predict\" data-toc-modified-id=\"Coding-Challenge:-predict-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Coding Challenge: <code>predict</code></a></span></li><li><span><a href=\"#Coding-Challenge:-score\" data-toc-modified-id=\"Coding-Challenge:-score-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Coding Challenge: <code>score</code></a></span></li><li><span><a href=\"#Coding-Challenge:-KNNClassifier\" data-toc-modified-id=\"Coding-Challenge:-KNNClassifier-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Coding Challenge: <code>KNNClassifier</code></a></span></li></ul></li><li><span><a href=\"#Discussion-and-Applications\" data-toc-modified-id=\"Discussion-and-Applications-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Discussion and Applications</a></span></li><li><span><a href=\"#Extensions\" data-toc-modified-id=\"Extensions-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Extensions</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-nearest neighbors (KNN) is a machine learning model with many variations used for a wide variety of analytics, classification, regression, and information retrieval tasks. In this notebook, we will develop our own version of KNN without outside help from machine learning libraries or frameworks. Let's begin by discussing how we will use KNN, what KNN does, and how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I love flowers, but I have a hard time distinguishing between different types of flowers. We're going to use a KNN model to distinguish between three different subspecies of iris flowers: *Iris Setosa*, *Iris Versicolor*, and *Iris Virginica*. Before discussing KNN in more depth, let's take a look at the data we'll be working with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Analyzing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to import and analyze our data, we'll use a library called **pandas**. Pandas is widely used by data scientists and machine learning engineers for its efficient data structures and data analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas library stores data sets in a structure called a **dataframe**. We'll talk more about dataframes in a moment. Run the code cell below to import our iris data and store it in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "iris_df = pd.read_csv('./data/iris.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pandas dataframe is a 2-dimensional structure, similar to an excel sheet or an SQL table. The `pd.read_csv` method reads the contents of a csv file and converts it into a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, we refer to each column of a data set or a dataframe as a **feature** of the data set. Each row of a data set represents an **observation**. Run the cell below to display the first five observations of our `iris_df` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our data set has four different features: *sepal length*, *sepal width*, *petal length*, and *petal width*.\n",
    "\n",
    "We will be using these four features to try to predict the *class* of each flower (Iris Setosa/Iris Versicolor/Iris Virginica). Because the last column represents the class we are trying to predict, we don't consider it a feature of the data set. In stead, we call it the **label**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use pandas to learn a little bit more about the data we are working with. Run the cell below to display summary statistics describing each feature of our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas gives us many nice analytical tools out of the box. We've used pandas to learn about the structure of our data and the features of our data set. Going forward, it will be easier for us to use and interact with our data as a NumPy array rather than a pandas dataframe. Run the cell below to create a NumPy array with all of our iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = iris_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check out the [pandas documentation](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html#pandas.DataFrame) for more information on pandas dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use most of the data in the `iris_data` dataframe to **train** our model. In machine learning parlance, training refers to the process of teaching our model how to make predictions. Unfortunately, we can't use all of our data to train our model. Why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we used all of our iris data to train our KNN model. We can think of our trained KNN model as a friend who's been studying irises in their laboratory but has never actually tried to apply their knowledge out in a meadow. One day, out on a picnic with our friend, we spot an iris in the field - we have no clue what type of iris it is. Our friend confidently claims it's an iris virginica. We'd like to believe our friend, but we have no proof they are any good at actually identifying types of irises - they've never proven to us that they can accurately identify the different subspecies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img title=\"Irises\" src=\"images/iris-field-sal-settecase.jpg\" style=\"width:500px;height:auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to trust our friends. This means we've got to ask them to prove their skill. Another friend of ours begins studying irises in their own laboratory. They make an order from the florist for 50 iris virginica, 50 iris setosa, and 50 iris verisocolor. We sneakily intercept their order and withhold 10 flowers of each subspecies. Our friend goes to work studying the 40 flowers of each subspecies they receive. Once our friend has trained up, we ask them to prove their skill by correctly classifying the 30 flowers we reserved. Since we know the subspecies of each flower we are testing our friend on, we can assess how accurately our friend performs on this test. If our friend performs well, we can trust them to classify flowers out on a picnic, even if we don't know the subspecies ahead of time. If our friend performs badly, we tell them to go back to the lab (or we bring a different friend on the picnic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a machine learning model, we will split our available data into **training data**, which we will use to train our model, and **test data**, which we will use to test how well our models work. Reserving test data is *vital* to machine learning work processes since it allows us to measure how well our models perform, and tells us how much we should trust our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps you're feeling clever. Perhaps you're thinking \"let's use all of our available data to train our model, and reuse some of that data to see how well it performs! That way I can use as much data as possible and still have a sense for how well my model works.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's address this proposition with another analogy. I've got a friend, Lisa, who has been studying to try to identify whether a given word is German or French. She's got a set of 100 flash cards in German and 100 flashcards in French that she's been studying. I want to know how well Lisa can distinguish French words from German words. If I only test her on her flash card words, all I know is that she's memorized the flash cards! I have no idea if Lisa could identify a French word that isn't on one of her cards. I have no idea whether Lisa's skills **generalize** to the broader population of French and German words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've got to separate our training data from our test data. And we better not let them mingle. And we better be hard asses about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, we want to reserve more data for training than for testing. It's pretty common to use 80% of your data for training and reserve the last 20% as testing data. There are a number of factors that might influence how much of your data set to reserve as test data. Ask yourself: *what are the effects of using less training data, and what are the effects of using less test data?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final note on dividing training data and test data: it's important to randomly shuffle the observations in a data set before splitting them into the training data set and the test data set. For example, if we naively used the first 80% of observations in `iris_data` as our training data, and reserved the last 20% as test data without shuffling first, all af our test data would be made up of iris virginica, and very few iris virginica would be in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully I've conviced you that it is *imperative* to separate out training data from test data. We will also want to separate the data set's *label* (i.e. the value we are trying to predict), from the data sets features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Why do we want to separate the label from the features?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we test a model, we want to make sure it isn't cheating. So we want to provide all of our test observations to the model without giving the model the labels it is trying to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also want to separate the labels in our training data. This will be convenient for us in the long run, but isn't strictly speaking necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Challenge: `train_test_split`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function description**\n",
    "\n",
    "`train_test_split` will begin by randomly shuffling the observations in our data set (you may want to use [np.random.shuffle](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.shuffle.html)). Then `train_test_split` will divide the data into for parts:\n",
    "\n",
    "1. The feature set for our training data\n",
    "1. The labels of our training data\n",
    "1. The feature set for our test data\n",
    "1. The labels of our test data\n",
    "\n",
    "See the image below for an example of how to divide a data set. You may assume the data set's label is in the last column of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/train_test_split_example.png\" style=\"width:500px;height:auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function input**\n",
    "1. `data`: NumPy array with shape `(n,m)`\n",
    "1. `test_size`: Float between 0.0 and 1.0 denoting how much of the data set to reserve as test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funciton output:**\n",
    "1. A tuple (X_train, y_train, X_test, y_test)\n",
    "    * `X_train` is a NumPy array that stores the feature set of the training observations. Should have shape `(n*(1 - test_size), m-1)`\n",
    "    * `y_train` is a NumPy array that holds the labels for all of the training set. Should have shape `(n*(1 - test_size),)`\n",
    "    * `X_test` is a NumPy array that stores the feature set of the test observations. Should have shape `(n*test_size, m-1)`\n",
    "    * `y_test` is a NumPy array that holds the labels for all of the test set. Should have shape `(n*test_size,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will probably want to use numpy methods for the challenge below\n",
    "# Feel free to import any modules you may want from the python standard library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, test_size=0.2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using your `train_test_split` function in the code cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to divide iris_data\n",
    "X_train, y_train, X_test, y_test = train_test_split(iris_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to write some simple test cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Custom KNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've prepared our data, let's talk about k-nearest neighbors! For the sake of explaining KNN, we'll use a simplified version of our iris data set. Our data set has two features, sepal length and petal length, and a label, setosa or versicolor or virginica. We've plotted the training data below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Iris data scatter plot](images/iris-scatter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two-dimensional coordinate plane above is called **feature space**, since each point in the space represents an observation with specific values for each feature of the data set. Suppose we come across an iris. We know it has a petal length of `5.35` centimeters and a sepal length of `3.35` centimeters. Let's place it in our feature space along with our training data, and see if it helps us make an educated guess about what type of iris it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Iris data scatter plot with unkown flower](images/unknown-iris.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the scatter graph above, we'd probably guess our unlabeled iris is an iris versicolor. *Why?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the three nearest neighbors to our unkown iris are all iris versicolor. This let's us make an educated guess that our unknown iris probably belongs to the versicolor subspecies. This also sheds light on how k-nearest neighbors works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three Nearest Neighbors for Iris Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a description of how a three nearest neighbors model classifies an unlabeled iris. We will impliment these steps in the coding challenges that follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start by storing all of the training data in memory.\n",
    "1. If asked to classify (i.e. predict the subspecies of) an unlabeled iris, consider the unlabeled iris as a point in the feature space.\n",
    "1. Calculate the distance between each point in the training data and our unlabeled iris.\n",
    "1. Identify the three closest points to the unlabeled iris.\n",
    "1. Let these three closest points \"vote\" on which class the unlabeled iris belongs to.\n",
    "1. In case of a three way tie, choose the class of the nearest neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Challenge: `get_distances`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Description**\n",
    "\n",
    "`get_distances` will accept to arguments: an unlabeled iris whose subspecies we don't know, called `observation`, and the training data we are using for our KNN model, called `X_train`. We can think of `observation` as a point in feature space, and we can think of `X_train` as an array of many points in feature space.\n",
    "\n",
    "Begin by creating a new array called `difference_vectors` that stores a list of . `difference_vectors` should have the same shape as `X_train`.\n",
    "\n",
    "Create a new array called `distances` that represents the length of each vector in `difference_vectors`. You should calculate the length of the difference vectors using the euclidean distance formula. For a difference vector `[x_0,x_1, ..., x_n]`, its euclidean length is:\n",
    "\n",
    "$$\\sqrt{\\sum_{i=0}^{n}x_i^2}$$\n",
    "\n",
    "Return the `distances` array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider an example. Suppose we've got an `observation`: `np.array([1,2])` and `X_train`: `np.array([[1,1], [2,3], [3,2]])`.\n",
    "\n",
    "Then `difference_vectors` would be `np.array([[0,-1], [1,1], [2,0]])`. We can interpret the difference vectors as the arrows in the graphic below:\n",
    "\n",
    "![]()\n",
    "\n",
    "and the return value, `distances`, would be `np.array([1, 1.414213, 2])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Input**\n",
    "\n",
    "1. `observation`: A numpy array with shape `(n,)` that represents an unlabeled observation\n",
    "1. `X_train`: A numpy array with shape `(m,n)` that represents our training data. You can think of `X_train` as a list of points in feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Output**\n",
    "\n",
    "1. `distances`: an array with shape `(m,)` that represents the distances between the `observation` and each point in `X_train`. For instance, the output of `get_distances(np.array([0,0]), np.array([[1,0], [0,0], [1,1]]))` should be `np.array([1, 0, 1.414213])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distances(observation, X_train):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to write your own tests for get_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Challenge: `get_nearest_neighbors`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Description**\n",
    "\n",
    "`get_nearest_neighbors` will accept a value `k`, representing the number of nearest neighbors to return, an unlabeled observation called `obs`, the training data features called `X_train`, and the corresponding labels for the training data called `y_train`.\n",
    "\n",
    "`get_nearest_neighbors` should use the `get_distances` function to calculate the distance between the unlabeled observation and each training observation in `X_train`. Next, `get_nearest_neighbors` should identify the indeces of the `k` nearest neighbors (consider using the method `np.argsort`). Using numpy slicing, return the labels of the `k` nearest neighbors from the `y_train` array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Input**\n",
    "\n",
    "1. `k`: an integer representing the number of nearest neighbors to return\n",
    "1. `obs`: numpy array with shape `(n,)` that represents an unlabeled observation. We are trying to find the labels of the `k` points in the training set that are closest to `obs`.\n",
    "1. `X_train`: numpy array with shape `(m,n)` that represents the training data (without labels).\n",
    "1. `y_train`: numpy array with shape `(m,)` that represents the labels for the training data in `X_train`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Output**\n",
    "\n",
    "1. `labels`: A numpy array with shape `(k,)` that holds the labels of the `k` nearest neighbors of `obs` in `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_neighbors(k, obs, X_train, y_train):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to write your own tests for get_nearest_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Challenge: `determine_class`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Description**\n",
    "\n",
    "`determine_class` accepts a single variable, `labels`, which is a NumPy array representing the labels of the k-nearest neighbors for some unlabeled observation. `determine_class` should return the label that appears most in `labels`. If their is a tie, the function should return the tied label which is closest to the unlabeled observation (i.e. earliest in the array)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Input**\n",
    "\n",
    "1. `labels`: NumPy array with shape `(k,)` that represents the labels of the k-nearest neighbors. The first element in the array is the label of the nearest neighbor while the last element in the array is the label of the $k^{th}$ nearest neighbor. The labels may be strings or numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Output**\n",
    "\n",
    "1. `class`: The string/number that appears most frequently in `labels`. If there is a tie, return the label that is tied for most appearances in `labels` and appears earliest in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to import any python standard library tools you may want!\n",
    "\n",
    "def determine_class(labels):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to write your own tests for determine_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Challenge: `predict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Description**\n",
    "\n",
    "`predict` will connect the different pieces we've been working on so far. The goal of `predict` is to predict the labels for an array of unlabeled data. You should use the `get_nearest_neighbors` and `determine_class` functions from earlier challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Input**\n",
    "\n",
    "1. `k`: An integer representing the number of nearest neighbors to take into account when making predictions.\n",
    "1. `X_train`: A NumPy array with shape `(m,n)` that represents the training data.\n",
    "1. `y_train`: A NumPy array with shape `(m,)` that represents the labels for the data in `X_train`.\n",
    "1. `X_test`: A NumPy array with shape `(j,n)` that represents unlabeled data whose labels we'd like to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Output**\n",
    "\n",
    "1. `predictions`: A NumPy aray with shape `(j,)` that represents the predicted labels for the observations in `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(k, X_train, y_train, X_test):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to write your own tests for predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Challenge: `score`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Description**\n",
    "\n",
    "We need a way to assess how well a classification algorithm is working. The `score` function will return the [accuracy]() of a KNN model. `score` will return the percentage of observations in `X_test` that are correctly classified. Your function should use the `predict` function you wrote in the preceding challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Input**\n",
    "\n",
    "1. `k`: An integer representing the number of nearest neighbors to take into account when making predictions.\n",
    "1. `X_train`: A NumPy array with shape `(m,n)` that represents the training data.\n",
    "1. `y_train`: A NumPy array with shape `(m,)` that represents the labels for the data in `X_train`.\n",
    "1. `X_test`: A NumPy array with shape `(j,n)` that represents our test data.\n",
    "1. `y_test`: A NumPy array with shape `(j,)` that represents the labels for the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Output**\n",
    "\n",
    "1. `accuracy`: A float between `0.0` and `1.0` representing the portion of correctly classified observations in `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(k, X_train, y_train, X_test, y_test):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to write your own tests for score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Challenge: `KNNClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge Description**\n",
    "\n",
    "We want to make our KNN model easy to use. It would be very convenient to group together the functions we've created as methods of a `KNNClassifier` python class.\n",
    "\n",
    "Complete the methods in the `KNNClassifier` class in the coding cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifier:\n",
    "    def __init__(self, k):\n",
    "        # Should store k as an attribute of self\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        # Should store X_train and y_train as attributes of self\n",
    "        # Should return a reference to self to allow method chaining\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        # Should use the predict function, as well as attributes of self,\n",
    "        # in order to return predictions\n",
    "        pass\n",
    "    \n",
    "    def score(self, X_test, y_test):\n",
    "        # Should use the score function, as well as attributes of self,\n",
    "        # in order to return the accuracy of the model\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to create a KNN Classifier and assess its accuracy on our iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_model = KNNClassifier(k=5).fit(X_train, y_train)\n",
    "# print(\"KNN accuracy on iris data:\", iris_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion and Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-nearest neighbors is a **supervised learning algorithm** since it requires *labeled data* in order for us to train it properly. There are unsupervised variations of KNN that are used for information retrieval and querying that don't require training data to be labeled. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-nearest neighbors is an **instance-based learning algorithm** since it needs to \"remember\" (i.e. store in memory) all of the training data in order to function properly. In order to make predictions, instance-based algorithms compare unseen/unlabeled data against the training instances they store in memory. As a consequence, instance-based algorithms tend to scale poorly (in space and time) as the size of the training data set grows. The more training data the algorithm has to remember, the more memory required to store that data, and the longer it takes to compare unlabeled data against the training instances in order to make a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the models we'll be discussing in the future are **parametric learning algorithms**. A parametric model doesn't need to store all of training data in memory. Instead, a parametric model uses training data to learn a set of parameters that help it make accurate predictions. After the training process is complete, the model only needs to store the parameters it has learned, rather than all of the training data it used in the process. Therefore, once a parametric model is trained, the memory required to store the model doesn't grow with a larger training set, nor does the time it takes to make predictions. However, parametric models can take a very long time to train up front. Instance-based models have no up front training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously stated, KNN is considered an instance-based model, **not** a parametric model. KNN does, however, have a special parameter - the value `k`. Since the value of `k` is picked prior to training a model, and isn't changed or updated during training, we call `k` a **hyperparameter**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few other behind-the-scenes hyperparameters for KNN that we haven't yet discussed. Changing these hyperparameters leads to creating subtly different variations of the KNN model. One such hyperparameter is our choice of **distance metric**. In the model we created above, we use Euclidean distance to evaluate the distance between points in feature space. Euclidean distance corresponds with how we humans tend to think about distance in our 3-dimensional world, but it's not the only valid way to calculate distance between two points. Instead we could create a KNN model that uses [uniform distance](https://en.wikipedia.org/wiki/Uniform_norm) or [taxi cab distance](https://en.wikipedia.org/wiki/Taxicab_geometry) to assess the distance between points. For more information on distance metrics, see [this link](https://en.wikipedia.org/wiki/Metric_%28mathematics%29)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In industry, KNN is most frequently used for information retrieval and searching algorithms. Spotify, for instance, uses a modified version of KNN called [annoy](https://github.com/spotify/annoy) to perform queries for spotify users, artists, and tracks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN excels relative to neural networks on small training sets. For this reason, KNN is sometimes used for complex tasks image classification if the training set is too small to reliably train deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you complete the preceding coding challenges to your satisfaction, you may move on to the optional extension problems below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a markdown cell below, and use it to write up responses to the questions below. Complete this before moving on to the other extensions.\n",
    "    1. As the hyperparameter k grows and becomes close to the number of observations in the training set, what happens to the KNN classifier? Are small or large values of k more likely to lead to overfitting?\n",
    "    1. Suppose we want to create a knn classifier for a data set of cars - we'd like to use the number of seats and the weight of a car in pounds to predict its safety grade. The two features in our data set (number of seats and weight) have wildly different [variances](https://en.wikipedia.org/wiki/Variance). What effects might this have on our KNN classifier? Are there alterations to the model or the data that might correct for these effects?\n",
    "    1. Compare and contrast the Euclidean distance, uniform distance, and taxi cab distance metrics. What are some reasons you might pick the taxi cab or uniform metric instead of the Euclidean metric? \n",
    "    1. Suppose we want to figure out the best possible choice for k (this is called hyperparameter tuning). Our naive approach is to try out a handful of values for k on the same training data/test data split to determine which value of k yields the best performance on the test data. We then choose this value of k to use for our model. This approach introduces a pernicious flaw in our model called data leakage. We have allowed information about the test data set to influence our choice of model (i.e. information about the test data has leaked into our training process and choice of model). As a result, the way our model performs on the test data can no longer give us an accurate idea of how well the model will behave on unseen data. This is because we have picked a model specifically because it performs well on the test data. Come up with one or more solutions for how to perform hyperparameter tuning without introducing data leakage. \n",
    "1. Implement [k-fold cross validation](https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29) to pick a value for the hyperparameter k.\n",
    "1. Train a KNN model on the iris data set using only the sepal length and sepal width features. Then, use the matplotlib library to create a visualization of how your KNN model classifies points. Your visualization might look something like this:\n",
    "\n",
    "![](images/knn-classification-regions.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
